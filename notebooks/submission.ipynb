{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "718e1c13",
   "metadata": {},
   "source": [
    "# Hamiltonian descent for Neural Networks\n",
    "\n",
    "Many commonly used optimization methods in Machine Learnign (*ML*) show linear to sublinear convergence rates. Hamiltonian Descent Methods (*HD*) are a new method that extend linear convergence for a broader class of convex functions (Maddison et al., 2018). We provide a method that implements HD in TensorFlow. We compare the method's performance to stochastic gradient descent (*SGD*) and adaptive moment estimation (*Adam*). We analyze performance using a convolutional neural network (*CNN*) using the MNIST dataset as well as using <span style=\"color:red\">...</span> .\n",
    "\n",
    "---\n",
    "\n",
    "This submission has mainly been prepared by Jannis Zeller (RWTH Aachen University), with help from David Schischke (University of Trier). \n",
    "\n",
    "## Table of Contents \n",
    "\n",
    "* [Theoretical Background](#theoretical-background)\n",
    "* [Implementation in TensorFlow](#implementation-in-tensorflow)\n",
    "* [Performance Analysis](#performance-analysis)\n",
    "  * [CNN: MNIST](#cnn-mnist)\n",
    "* [Discussion](#disccusion)\n",
    "* [Literature](#literature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c157ddb",
   "metadata": {},
   "source": [
    "## Theoretical Background \n",
    "\n",
    "This section provides a brief summary of the HD methods first proposed by [Maddison et al. (2018)](https://arxiv.org/abs/1809.05042), adding some context and comparisons to commonly used optimization methods for Neural Networks. We first present a brief introduction of the terms we will use subsequently whereas, for the sake of understandability, we try to use as little mathematical notation as possible. \n",
    "\n",
    "### Hamiltonian Mechanics\n",
    "\n",
    "<span style=\"color:orange\"> @Jannis: Kurze Intro was die machen lol </span>\n",
    "\n",
    "### Optimization basics\n",
    "\n",
    "Optimization methods are characterized by several requirements they place on the function that needs to be optimized (*objective function*), some of which are of relevance for the understanding of HD. For more concise definitions, we refer to [Bierlaire (2015)](http://optimizationprinciplesalgorithms.com/) for a well-written and (comparably) easy to understand introductory textbook on optimization that is freely available.\n",
    "\n",
    "* **Convexity of the Objective Function**: Convex functions are a family of functions that have desirable properties for optimization methods, mainly the fact that any local optimum we find is certain to be a global optimum as well. In brief, convex functions describe the family of functions for which the connecting line between two points is above the function values at every point. All optimization methods in this notebook are designed to work on convex functions.\n",
    "* **Constrainedness of the Objective Function**: An optimization is constrained if the objective function is restricted by equality requirements (e.g. $\\min f(x) \\text{ s.t. } c(x) = 0$) or by inequality requirements (e.g. $\\min f(x) \\text{ s.t. } c(x) \\leq 0$). All optimization methods presented in this notebook perform unconstrained optimization.\n",
    "* **Differentiability of the Objective Function**: Optimization methods require different degrees of differentiability of the objective function. All optimization methods presented in this notebook require information about the gradient of the objective function, which means that the objective function must be differentiable once (i.e. $f \\in C^1$).\n",
    "\n",
    "In addition to the differences in requirements on the objective function, optimization algorithms differ in their efficiency in terms of iterations $k$ needed to find an optimal solution $\\hat x$ (so-called *convergence rate*). We can differentiate four orders of convergence, ranked from fastest to slowest: \n",
    "\n",
    "1. **Quadratic**: $\\lVert x^{k+1} - \\hat x\\rVert \\leq C\\lVert x^k - \\hat x\\rVert^2$ for some $C< \\infty$\n",
    "2. **Superlinear**: $\\lVert x^{k+1} - \\hat x \\rVert \\leq \\alpha_k \\lVert x^k - \\hat x\\rVert$ for $\\alpha_k \\downarrow 0$\n",
    "3. **Linear**: $\\lVert x^{k+1} - \\hat x\\rVert\\leq \\alpha \\lVert x^k - \\hat x \\rVert$ for some $\\alpha < 1$\n",
    "4. **Sublinear**: $\\lVert x^k - \\hat x\\rVert \\leq \\frac C{k^\\beta}$, often $\\beta \\in \\{2,1,\\frac12\\}$\n",
    "\n",
    "### Hamiltonian Descent methods\n",
    "\n",
    "This section is a summary of the HD methods proposed by Maddison et al. (2018). HD methods describe a set of first-order unconstrained optimization methods (i.e. $f \\in C^1$) for convex functions. By incorporating the Hamiltonian Framework, it is possible to use the kinetic energy $k(p_t)$ (with $p_t$ being the momentum of $x$ at time $t$) and it's respective $\\nabla k$ to obtain additional information about the objective function $f$. In order to be able to obtain linear convergence, the kinetic energy must be chosen proportional to the convex conjugate of $f(x): k(p) \\propto f^*(p) + f^*(-p)$ (with $f^*$ being the convex conjugate of $f$, for intuition see [Le Priol, 2020](https://remilepriol.github.io/dualityviz/)). This assumption can be relaxed to $k(p)\\geq \\alpha \\max\\{f^*(p), f^*(-p)\\}, \\; 0 < \\alpha \\leq 1$ while maintaining linear convergence. Furthermore, depending on the nature of $f$, $k$ must be chosen appropriately to ensure linear convergence.\n",
    "\n",
    "An apparent benefit of the *HD* method is that it achieves linear convergence while using a fixed step size. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For neural networks, the most commonly used method is SGD.\n",
    "\n",
    "* SGD: Sublinear convergence $\\mathcal{O} \\left(\\dfrac{1}{k}\\right)$ with $k = 1, ..., \\infty$ being the iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb4b02",
   "metadata": {},
   "source": [
    "## Implementation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "#-------------------------------------------------------------------------------\n",
    "import warnings\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# %% Implementation\n",
    "# Implementation of custom tf/tf.keras optimizer inspired by https://cloudxlab.com/blog/writing-custom-optimizer-in-tensorflow-and-keras/\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "class KineticEnergyGradients():\n",
    "    \"\"\"This class provides several functions which serve as kinetic energy gradients in Hagrad.\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def classical(p_var: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Classical kinetic energy ||p||^2/2 with gradient p.\"\"\"\n",
    "        return(p_var)\n",
    "\n",
    "    @tf.function\n",
    "    def relativistic(p_var: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Relativistic kinetic energy sqrt( ||p||^2 + 1 )-1 with gradient p/sqrt( ||p||^2 + 1 )\"\"\"\n",
    "        return(p_var / tf.math.sqrt(tf.math.square(tf.norm(p_var)) + 1.))\n",
    "\n",
    "    def power(self, power_a=2., power_A=1.) -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "        \"\"\"Power kinetic energy (1/A) * ( ||p||^a + 1 )^(A/a) - (1/A) with gradient p * ||p||^(a-2) * ( ||p||^a + 1 )^(A/a-1)\"\"\"\n",
    "        a = float(power_a)\n",
    "        A = float(power_A)\n",
    "        @tf.function\n",
    "        def power_func(p_var: tf.Tensor) -> tf.Tensor:\n",
    "            #\"\"\"Power kinetic energy (1/A) * ( ||p||^a + 1 )^(A/a) - (1/A) with gradient p * ||p||^(a-2) * ( ||p||^a + 1 )^(A/a-1)\"\"\"\n",
    "            p_norm = tf.norm(p_var)\n",
    "            return(p_var*p_norm**(a-2.) * (p_norm**a + 1.)**(A/a-1.))\n",
    "        power_func.__doc__ = f\"Power kinetic energy (1/{A}) * ( ||p||^{a} + 1 )^({A}/{a}) - (1/{A}) with gradient p * ||p||^({a}-2) * ( ||p||^{a} + 1 )^({A}/{a}-1)\"\n",
    "        return power_func\n",
    "\n",
    "    @tf.function\n",
    "    def power_1norm(self, p_var: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Power kinetic energy (1/A) * ( ||p||^a + 1 )^(A/a) - (1/A) with gradient p * ||p||^(a-2) * ( ||p||^a + 1 )^(A/a-1) where ||p|| is the 1-norm.\"\"\"\n",
    "        a = self.power_a\n",
    "        A = self.power_A\n",
    "        p_norm = tf.norm(p_var, ord=1)\n",
    "        return(tf.math.sign(p_var)*p_norm**(a-1.) * (p_norm**a + 1.)**(A/a-1.))\n",
    "\n",
    "\n",
    "class Hagrad(keras.optimizers.Optimizer):\n",
    "    _significant_decimals_hypers: int=4\n",
    "\n",
    "    def __init__(self, \n",
    "        epsilon: float=1., \n",
    "        gamma:   float=10., \n",
    "        name:    str=\"hagrad\", \n",
    "        kinetic_energy_gradient: Callable[[tf.Tensor], tf.Tensor]=KineticEnergyGradients.relativistic,\n",
    "        p0_mean: float=1.,\n",
    "        p0_std:  float=2.,\n",
    "\n",
    "        **kwargs\n",
    "    ): \n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self.kinetic_energy_gradient = kwargs.get(\"kinetic_energy_gradient\", kinetic_energy_gradient)\n",
    "        self._set_hyper(\"epsilon\", kwargs.get(\"lr\", epsilon)) \n",
    "        self._set_hyper(\"gamma\", kwargs.get(\"gamma\", gamma))\n",
    "        self._set_hyper(\"p0_mean\", kwargs.get(\"p0_mean\", p0_mean)) \n",
    "        self._set_hyper(\"p0_std\", kwargs.get(\"p0_std\", p0_std))\n",
    "\n",
    "        delta =  1. / (1. + kwargs.get(\"lr\", epsilon)*kwargs.get(\"gamma\", gamma) )\n",
    "        n = self._significant_decimals_hypers -int(np.floor(np.log10(abs(delta))))-1\n",
    "        self._set_hyper(\"delta\", round(delta, n))\n",
    "\n",
    "        eps_delta = epsilon*delta\n",
    "        n = self._significant_decimals_hypers -int(np.floor(np.log10(abs(eps_delta))))-1\n",
    "        self._set_hyper(\"eps_delta\", round(eps_delta, n))\n",
    "\n",
    "        if self.kinetic_energy_gradient.__name__ == \"power_1norm\":\n",
    "            warnings.warn(\"The power_1norm kinetic energy gradient typically leads to divergence!\")\n",
    "    \n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        var_dtype = var_list[0].dtype.base_dtype\n",
    "\n",
    "        ## Initializing kinetic energy for t=0.\n",
    "        p0_mean = self._get_hyper(\"p0_mean\", var_dtype)\n",
    "        p0_std  = self._get_hyper(\"p0_std\", var_dtype)\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"hamilton_momentum\", tf.random_normal_initializer(mean=p0_mean, stddev=p0_std)) \n",
    "        ## Checkup\n",
    "        # print(self._weights)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        p_var = self.get_slot(var, \"hamilton_momentum\")\n",
    "        epsilon = self._get_hyper(\"epsilon\", var_dtype)\n",
    "        delta   = self._get_hyper(\"delta\", var_dtype)\n",
    "        eps_delta = self._get_hyper(\"eps_delta\", var_dtype)\n",
    "\n",
    "\n",
    "        p_var.assign(delta * p_var - eps_delta * grad)\n",
    "        var.assign_add(epsilon * self.kinetic_energy_gradient(p_var))\n",
    "\n",
    "\n",
    "        # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
    "        ## -> It seems like some (small amount of) computation time can be saved, if one does \n",
    "        #  not use conditions or a property to store different kinetic energy gradients.\n",
    "        # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
    "\n",
    "\n",
    "        # var.assign_add(epsilon * p_var) \n",
    "\n",
    "        ## DEPRECATED\n",
    "        # if self.kinetic_energy == \"classical\":\n",
    "        #     var.assign_add(epsilon * p_var)\n",
    "\n",
    "        # if self.kinetic_energy == \"relativistic\":\n",
    "        #     var.assign_add(epsilon * p_var / tf.math.sqrt(tf.math.square(tf.norm(p_var)) + 1.))\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            'epsilon': self._serialize_hyperparameter(\"epsilon\"),\n",
    "            'gamma':   self._serialize_hyperparameter(\"gamma\"),\n",
    "            'delta':   self._serialize_hyperparameter(\"delta\"),\n",
    "            'kinetic_energy_gradient': self.kinetic_energy_gradient.__doc__\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855513d",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "We first analyze the performance of our Hamiltonian Descent method using a standard CNN and the MNIST dataset.\n",
    "\n",
    "For comparison, we will use the SGD, as it is both the easiest and most implemented optimization method <span style=\"color:red\"> (Quelle) </span> and Adam, as it is ... .\n",
    "\n",
    "* Erwähnen, dass SGD stochatstisch ist und so zwar schneller, aber nur sublinear convergence\n",
    "\n",
    "### CNN: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426521f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST als Kaggle Dataset -> Im Online Mode unter \"Input -> Competitions -> MNIST\" Daten hinzufügen\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/digit-recognizer'):\n",
    "    for filename in filenames:\n",
    "        mnist_train = pd.read_csv(f\"{dirname}/train.csv\")\n",
    "        mnist_test = pd.read_csv(f\"{dirname}/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277ee10",
   "metadata": {},
   "source": [
    "### Other dataset\n",
    "\n",
    "Vielleicht sentiment analysis mit Twitter Daten? Aber ich kenn die klassischen Benchmark Datensätze nicht :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8ad21",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "* Wäre es möglich, stochastic Hamiltonian Descent zu machen um Laufzeit zu verbessern?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58f96a",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "Bierlaire, M. (2015). *Optimization: Principles and Algorithms*. EPFL Press.\n",
    "\n",
    "Le Priol, Remi. (2020). Visualizing Convex Conjugates. https://remilepriol.github.io/dualityviz/ \n",
    "\n",
    "Maddison, C. J., Paulin, D., Teh, Y. W., O'Donoghue, B., & Doucet, A. (2018). Hamiltonian descent methods. arXiv preprint c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27baac5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9da7387ac238462230b0dbd9bf3fc9372aae1b82f76d8141914412c04a1575e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
