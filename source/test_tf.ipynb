{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to implement Hamiltonian Descent Method with `tf.keras`\n",
    "\n",
    "Lets call it \"hagrad\" for **Ha**miltonian **Gr**adient **D**escent. LoL.\n",
    "\n",
    "Implementation of custom `tf`/`tf.keras` optimizer inspired by [https://cloudxlab.com/blog/writing-custom-optimizer-in-tensorflow-and-keras/](https://cloudxlab.com/blog/writing-custom-optimizer-in-tensorflow-and-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "#  Tensorflow related\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#  General\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hagrad(keras.optimizers.Optimizer):\n",
    "    def __init__(self, epsilon=0.001, gamma=1e7, name=\"hagrad\", kinetic_energy=\"classical\", **kwargs): \n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self.kinetic_energy = kwargs.get(\"kinetic_energy\", kinetic_energy)\n",
    "        self._set_hyper(\"epsilon\", kwargs.get(\"lr\", epsilon)) \n",
    "        self._set_hyper(\"gamma\", kwargs.get(\"gamma\", gamma))\n",
    "        self._set_hyper(\"delta\", 1. / (1. + kwargs.get(\"lr\", epsilon)*kwargs.get(\"gamma\", gamma) ))\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"hamilton_momentum\") \n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        p_var = self.get_slot(var, \"hamilton_momentum\")\n",
    "        epsilon = self._get_hyper(\"epsilon\", var_dtype)\n",
    "        delta   = self._get_hyper(\"delta\", var_dtype)\n",
    "        p_var.assign(delta * p_var - epsilon * delta * grad)\n",
    "\n",
    "        if self.kinetic_energy == \"classical\":\n",
    "            var.assign_add(epsilon * p_var)\n",
    "\n",
    "        if self.kinetic_energy == \"relativistic\":\n",
    "            var.assign_add(epsilon * p_var / tf.math.sqrt(tf.math.square(tf.norm(p_var)) + 1.))\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"epsilon\": self._serialize_hyperparameter(\"epsilon\"),\n",
    "            \"gamma\":   self._serialize_hyperparameter(\"gamma\"),\n",
    "            \"delta\":   self._serialize_hyperparameter(\"delta\"),\n",
    "            \"kinetic_energy\": self.kinetic_energy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, name=\"SGOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._is_first = True\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"pv\") #previous variable i.e. weight or bias\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"pg\") #previous gradient\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        new_var_m = var - grad * lr_t\n",
    "        pv_var = self.get_slot(var, \"pv\")\n",
    "        pg_var = self.get_slot(var, \"pg\")\n",
    "        \n",
    "        if self._is_first:\n",
    "            self._is_first = False\n",
    "            new_var = new_var_m\n",
    "        else:\n",
    "            cond = grad*pg_var >= 0\n",
    "            print(cond)\n",
    "            avg_weights = (pv_var + var)/2.0\n",
    "            new_var = tf.where(cond, new_var_m, avg_weights)\n",
    "        pv_var.assign(var)\n",
    "        pg_var.assign(grad)\n",
    "        var.assign(new_var)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "        }\n",
    "\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'hagrad', 'epsilon': 0.001, 'gamma': 1.5, 'delta': 0.9985022466300548, 'kinetic_energy': 'relativistic'}\n"
     ]
    }
   ],
   "source": [
    "hagrad_opt = Hagrad(epsilon=0.001, gamma=1.5, kinetic_energy=\"relativistic\")\n",
    "print(hagrad_opt.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 3.7378\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4864\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9196\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7603\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7058\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6775\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6608\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6483\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6354\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6241\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6166\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6061\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6007\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5940\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5877\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5819\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5768\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5728\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5681\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5641\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5613\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5586\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5566\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5533\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5500\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5486\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5477\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5453\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5442\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5416\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5411\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5399\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5405\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5386\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5373\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5370\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5361\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5355\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5350\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5332\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5340\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5331\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5319\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5318\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5333\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5322\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5320\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5301\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5317\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5312\n"
     ]
    }
   ],
   "source": [
    "model_sgd  = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\n",
    "model_sgd.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3)) \n",
    "fit_sgd = model_sgd.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2497\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.7778\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.7976\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0976\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.6026A: 0s - los\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2637\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0410\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8984\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8072\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7477\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7039\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6698\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6416\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6178\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5975\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5808\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5673\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5563\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5472\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5406\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5355\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5322\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5308\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5297\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5286\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5283\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5286\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5280\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5279\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5275\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5274\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5274\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5277\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5279\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5274\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5273\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5273\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5272\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5274\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5266\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5271\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5268\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5266\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5264\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5273\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5269\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5272\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5266\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5271A: 0s \n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5269\n"
     ]
    }
   ],
   "source": [
    "model_adam = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\n",
    "model_adam.compile(loss=\"mse\", optimizer=keras.optimizers.Adam()) \n",
    "fit_adam = model_adam.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.9105\n",
      "Epoch 2/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.3083\n",
      "Epoch 3/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.8455\n",
      "Epoch 4/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.8544\n",
      "Epoch 5/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2485\n",
      "Epoch 6/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8832\n",
      "Epoch 7/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6595\n",
      "Epoch 8/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5521\n",
      "Epoch 9/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5460\n",
      "Epoch 10/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5679\n",
      "Epoch 11/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5715\n",
      "Epoch 12/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5573\n",
      "Epoch 13/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5412\n",
      "Epoch 14/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5314\n",
      "Epoch 15/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5281\n",
      "Epoch 16/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5276\n",
      "Epoch 17/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5277\n",
      "Epoch 18/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5277\n",
      "Epoch 19/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5272\n",
      "Epoch 20/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5270\n",
      "Epoch 21/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5266\n",
      "Epoch 22/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5263\n",
      "Epoch 23/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5268\n",
      "Epoch 24/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5262\n",
      "Epoch 25/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5263\n",
      "Epoch 26/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5259\n",
      "Epoch 27/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5262\n",
      "Epoch 28/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5257\n",
      "Epoch 29/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5256\n",
      "Epoch 30/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5259\n",
      "Epoch 31/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5256\n",
      "Epoch 32/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5256\n",
      "Epoch 33/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5255\n",
      "Epoch 34/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5255\n",
      "Epoch 35/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5254\n",
      "Epoch 36/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5257\n",
      "Epoch 37/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5254\n",
      "Epoch 38/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5252\n",
      "Epoch 39/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5252\n",
      "Epoch 40/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5255\n",
      "Epoch 41/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5251\n",
      "Epoch 42/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5252\n",
      "Epoch 43/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5254\n",
      "Epoch 44/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5254\n",
      "Epoch 45/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5251\n",
      "Epoch 46/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5251\n",
      "Epoch 47/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5256\n",
      "Epoch 48/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5255\n",
      "Epoch 49/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5250\n",
      "Epoch 50/50\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5251\n"
     ]
    }
   ],
   "source": [
    "model_hagrad = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\n",
    "model_hagrad.compile(loss=\"mse\", optimizer=hagrad_opt) # SGOptimizer hagrad\n",
    "fit_hagrad = model_hagrad.fit(X_train_scaled, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_sgd  = fit_sgd.history[\"loss\"]\n",
    "loss_history_adam = fit_adam.history[\"loss\"]\n",
    "loss_history_hagrad = fit_hagrad.history[\"loss\"]\n",
    "epoch_history = np.arange(len(loss_history_hagrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(epoch_history, loss_history_sgd, color=\"r\", label=\"keras.optimizers.SGD\") \n",
    "plt.scatter(epoch_history, loss_history_adam, color=\"g\", label=\"keras.optimizers.Adam\") \n",
    "plt.scatter(epoch_history, loss_history_hagrad, color=\"k\", label=\"Hagrad\")  \n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss - MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2a35896fa9730502da6af1aea083e21fdd9e65ddb8689fcbeb076a8d96ff2976"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorbrot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
